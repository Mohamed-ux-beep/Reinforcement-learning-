{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeFF54r0Df2iIDORGKoZ+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-ux-beep/Reinforcement-learning-/blob/main/mountaincar1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diSgQ11lov8j",
        "outputId": "aa548c57-b448-4333-c43e-7812cbd7ce7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvrdHo5HopP8"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "env = gym.make('MountainCar-v0', render_mode='human')\n",
        "\n",
        "# Duel double Deep Quality Network DDDQN\n",
        "class DDDQN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(DDDQN, self).__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.d2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.v = tf.keras.layers.Dense(1, activation=None)\n",
        "        self.a = tf.keras.layers.Dense(env.action_space.n, activation=None)\n",
        "\n",
        "    def call(self, input_data):\n",
        "        x = self.d1(input_data)\n",
        "        x = self.d2(x)\n",
        "        v = self.v(x)\n",
        "        a = self.a(x)\n",
        "        Q = v + (a - tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
        "        return Q\n",
        "\n",
        "    def advantage(self, state):\n",
        "        x = self.d1(state)\n",
        "        x = self.d2(x)\n",
        "        a = self.a(x)\n",
        "        return a\n",
        "\n",
        "# Experience Replay class\n",
        "class exp_replay():\n",
        "    def __init__(self, buffer_size=1000000):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.state_mem = np.zeros((self.buffer_size, *(env.observation_space.shape)), dtype=np.float32)\n",
        "        self.action_mem = np.zeros((self.buffer_size), dtype=np.int32)\n",
        "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
        "        self.next_state_mem = np.zeros((self.buffer_size, *(env.observation_space.shape)), dtype=np.float32)\n",
        "        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool_)\n",
        "        self.pointer = 0\n",
        "\n",
        "    def add_exp(self, state, action, reward, next_state, done):\n",
        "        idx = self.pointer % self.buffer_size\n",
        "        self.state_mem[idx] = state\n",
        "        self.action_mem[idx] = action\n",
        "        self.reward_mem[idx] = reward\n",
        "        self.next_state_mem[idx] = next_state\n",
        "        self.done_mem[idx] = 1 - int(done)\n",
        "        self.pointer += 1\n",
        "\n",
        "    def sample_exp(self, batch_size=64):\n",
        "        max_mem = min(self.pointer, self.buffer_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "        states = self.state_mem[batch]\n",
        "        actions = self.action_mem[batch]\n",
        "        rewards = self.reward_mem[batch]\n",
        "        next_states = self.next_state_mem[batch]\n",
        "        dones = self.done_mem[batch]\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "# now the agent\n",
        "class agent():\n",
        "    def __init__(self, gamma=0.99, replace=100, lr=0.001):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1.0\n",
        "        self.min_epsilon = 0.01\n",
        "        self.epsilon_decay = 1e-3\n",
        "        self.replace = replace\n",
        "        self.trainstep = 0\n",
        "        self.memory = exp_replay()\n",
        "        self.batch_size = 64\n",
        "        self.q_net = DDDQN()  # algorithm duel double deep quality network\n",
        "        self.target_net = DDDQN()\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        self.q_net.compile(loss='mse', optimizer=opt)\n",
        "        self.target_net.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "    # update target to update our target model, exploration/ exploitation trade off\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice([i for i in range(env.action_space.n)])\n",
        "        else:\n",
        "            actions = self.q_net.advantage(np.array([state]))\n",
        "            action = np.argmax(actions)\n",
        "            return action\n",
        "\n",
        "    def update_mem(self, state, action, reward, next_state, done):\n",
        "        self.memory.add_exp(state, action, reward, next_state, done)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.set_weights(self.q_net.get_weights())\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = self.epsilon - self.epsilon_decay\n",
        "        return self.epsilon\n",
        "\n",
        "    def train(self):\n",
        "        if self.memory.pointer < self.batch_size:\n",
        "            return\n",
        "        if self.trainstep % self.replace == 0:\n",
        "            self.update_target()\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
        "        target = self.q_net.predict(states)\n",
        "        next_state_val = self.target_net.predict(next_states)\n",
        "        max_action = np.argmax(self.q_net.predict(next_states), axis=1)\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "        q_target = np.copy(target)\n",
        "        q_target[batch_index, actions] = rewards + self.gamma * next_state_val[batch_index, max_action] * dones\n",
        "        self.q_net.train_on_batch(states, q_target)\n",
        "        self.update_epsilon()\n",
        "        self.trainstep += 1\n",
        "\n",
        "\n",
        "# The training of the agent\n",
        "MountainCar = agent()\n",
        "steps = 400\n",
        "for s in range(steps):\n",
        "    done = False\n",
        "    state = env.reset()[0]\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = MountainCar.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        MountainCar.update_mem(state, action, reward, next_state, done)\n",
        "        MountainCar.train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print(\"total reward after {} episode is {} and epsilon ist {}\".format(s, total_reward, MountainCar.epsilon))"
      ]
    }
  ]
}